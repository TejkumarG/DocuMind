⚙️ Final DSPy RAG Pipeline — Full Architecture (with Flow Graph)

⸻

🧩 1️⃣  What Goes In

Component	Description
Input	question (from user)
Context (max 6)	Retrieved via your 3 + 3 strategy: semantic + filtered semantic
Goal	Generate a direct, factual answer with 2 LLM calls (Reason + Verify)


⸻

⚙️ 2️⃣  System Components

File	Role
retriever.py	Fetch 6 unique context chunks (3 + 3 pattern, deduplicated)
rag_pipeline.py	Defines 2-stage DSPy Reason + Verify logic
compile_dspy.py	Compiles (trains) DSPy with 100 gold examples
open_service.py	Runtime entry point → load compiled DSPy → answer queries
training_samples.json	100 records { question, context, answer } for compile
compiled_rag_v1.dspy	Serialized optimized DSPy object (your “brain”)


⸻

🧩 3️⃣  Flow Graph — Full Lifecycle

         ┌──────────────────────────────────────────────┐
         │                OFFLINE PHASE (Compile)       │
         └──────────────────────────────────────────────┘
                               │
        ┌────────────────────────────────────────────────────────┐
        │ 1️⃣ Prepare 100 samples (question, context, answer)     │
        │    →  data/training_samples.json                       │
        └────────────────────────────────────────────────────────┘
                               │
        ┌────────────────────────────────────────────────────────┐
        │ 2️⃣ Run compile_dspy.py                                │
        │    dspy.settings.configure(OpenAI...)                  │
        │    rag = ReasonVerifyRAG()                             │
        │    compiler = MIPROv2(metric=answer_faithfulness)      │
        │    compiled = compiler.compile(rag, trainset=100)      │
        │    compiled.save("models/compiled_rag_v1.dspy")        │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
         ┌──────────────────────────────────────────────┐
         │     🧠 compiled_rag_v1.dspy saved to /models  │
         │     (optimized Reason + Verify prompt logic)  │
         └──────────────────────────────────────────────┘
                               │
                               ▼
         ┌──────────────────────────────────────────────┐
         │             ONLINE PHASE (Runtime)            │
         └──────────────────────────────────────────────┘
                               │
        ┌────────────────────────────────────────────────────────┐
        │ 1️⃣  Receive new user question                         │
        │     → "Who founded Tesla Motors?"                      │
        └────────────────────────────────────────────────────────┘
                               │
        ┌────────────────────────────────────────────────────────┐
        │ 2️⃣  Run retriever.get_context(question)               │
        │     → returns max 9 chunks (3+3+3 unique)              │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
        ┌────────────────────────────────────────────────────────┐
        │ 3️⃣  Load compiled DSPy object                         │
        │     rag = dspy.load("models/compiled_rag_v1.dspy")     │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
        ┌────────────────────────────────────────────────────────┐
        │ 4️⃣  Configure runtime model (OpenAI 4o-mini)          │
        │     dspy.settings.configure(lm=dspy.OpenAI(...))       │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
        ┌────────────────────────────────────────────────────────┐
        │ 5️⃣  Two LLM Calls via Reason + Verify                │
        │     a. reason_result = rag.reason(context, question)   │
        │         → 1st LLM call = Generate draft answer         │
        │     b. verify_result = rag.verify(context, answer)     │
        │         → 2nd LLM call = Check and correct             │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
        ┌────────────────────────────────────────────────────────┐
        │ 6️⃣  Return verify_result → Direct Factual Answer      │
        │     e.g., "Tesla Motors was founded by Martin Eberhard│
        │     and Marc Tarpenning."                              │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
        ┌────────────────────────────────────────────────────────┐
        │ 7️⃣  If user 👍 likes → store (question, context, answer)│
        │     to data/feedback.jsonl for future re-compile        │
        └────────────────────────────────────────────────────────┘
                               │
                               ▼
        ┌────────────────────────────────────────────────────────┐
        │ 8️⃣  Periodically re-run compile_dspy.py with 200+ good│
        │     examples → new compiled_rag_v2.dspy                │
        └────────────────────────────────────────────────────────┘


⸻

⚙️ 4️⃣  Key Technical Facts

Aspect	Value / Explanation
Input per query	question + max 9 unique context chunks
Model calls per query	2 (Reason + Verify)
LLM configured via	dspy.settings.configure(lm=dspy.OpenAI(...))
Compile cost (100 examples)	~300 – 600 calls (cheap on 4o-mini)
Runtime cost	~2 calls × 1.5 k tokens ≈ <$0.001 / query
Compiled object path	models/compiled_rag_v1.dspy
Recompile frequency	When feedback ≥ 50 new good examples (weekly or on-demand)


⸻

🧠 5️⃣  Quick Command Flow

# 1️⃣ Compile the “brain” once (offline)
python app/compile_dspy.py

# 2️⃣ Ask questions anytime (runtime)
python app/open_service.py

# 3️⃣ Add new liked answers
#    Then recompile with updated training_samples.json
python app/compile_dspy.py
