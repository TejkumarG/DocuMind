# DSPy RAG Pipeline - Complete Architecture

## ğŸ“‹ Table of Contents
1. [System Overview](#system-overview)
2. [Application Flow](#application-flow)
3. [File Structure & Responsibilities](#file-structure--responsibilities)
4. [Request Flow Diagram](#request-flow-diagram)
5. [LLM Call Sequence](#llm-call-sequence)
6. [Data Flow](#data-flow)
7. [Component Details](#component-details)

---

## System Overview

**DSPy RAG Pipeline** is a two-stage Retrieval-Augmented Generation system that:
- Retrieves relevant context from your document store
- Uses DSPy's Reason + Verify pattern with 2 LLM calls
- Returns verified, accurate answers to questions

### Key Features
- âœ… **2 LLM calls per query**: Reason (draft) â†’ Verify (correct)
- âœ… **Context retrieval**: Integrates with external retrieval API
- âœ… **OpenAI GPT-4o-mini**: Fast, cheap, accurate
- âœ… **Optional training**: Can be optimized with DSPy compilation
- âœ… **Feedback loop**: Store liked answers for future training

---

## Application Flow

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ POST /api/v1/ask
                     â”‚ {"question": "..."}
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Application (Port 8001)                â”‚
â”‚                     app.py + api/routes.py                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                          â”‚
        â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retrieval API   â”‚    â”‚  OpenAI API          â”‚
â”‚  (Port 8000)     â”‚    â”‚  (gpt-4o-mini)       â”‚
â”‚                  â”‚    â”‚                      â”‚
â”‚  Returns 3-6     â”‚    â”‚  2 LLM Calls:        â”‚
â”‚  context chunks  â”‚    â”‚  1. Reason (draft)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  2. Verify (final)   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure & Responsibilities

```
reasoning/
â”‚
â”œâ”€â”€ app.py                              # FastAPI app entry point
â”‚   â””â”€> Creates FastAPI instance
â”‚   â””â”€> Includes routes from api/routes.py
â”‚   â””â”€> Configures CORS
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes.py                       # API endpoint handlers
â”‚       â”œâ”€> /ask          â†’ Main question answering endpoint
â”‚       â”œâ”€> /feedback     â†’ Store user feedback
â”‚       â”œâ”€> /compile      â†’ Trigger DSPy compilation
â”‚       â”œâ”€> /recompile    â†’ Recompile with feedback
â”‚       â””â”€> /health       â†’ Health check
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                     # Configuration management
â”‚       â””â”€> Loads .env variables
â”‚       â””â”€> OpenAI API key, model, temperature
â”‚       â””â”€> File paths for models, data
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py                      # Pydantic request/response models
â”‚       â”œâ”€> QuestionRequest
â”‚       â”œâ”€> AnswerResponse
â”‚       â”œâ”€> FeedbackRequest
â”‚       â”œâ”€> CompileRequest
â”‚       â””â”€> CompileResponse
â”‚
â”œâ”€â”€ handlers/
â”‚   â”‚
â”‚   â”œâ”€â”€ runtime/                        # ONLINE PHASE (per query)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ 2_get_context.py           # Retrieve context chunks
â”‚   â”‚   â”‚   â””â”€> Calls retrieval API (localhost:8000/retrieve)
â”‚   â”‚   â”‚   â””â”€> Extracts "text" from response chunks
â”‚   â”‚   â”‚   â””â”€> Returns List[str] (3-6 chunks)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ 3_load_dspy.py             # Load DSPy model
â”‚   â”‚   â”‚   â””â”€> Tries to load compiled model from artifacts/
â”‚   â”‚   â”‚   â””â”€> Falls back to ReasonVerifyRAG() if not found
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ 4_configure_model.py       # Configure OpenAI
â”‚   â”‚   â”‚   â””â”€> Sets up dspy.OpenAI with API key
â”‚   â”‚   â”‚   â””â”€> Configures temperature, model name
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ 5_reason_verify.py         # Two-stage RAG pipeline
â”‚   â”‚   â”‚   â”œâ”€> ReasonSignature: Generate draft answer
â”‚   â”‚   â”‚   â”œâ”€> VerifySignature: Verify and correct
â”‚   â”‚   â”‚   â””â”€> ReasonVerifyRAG.forward(): Execute both stages
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ 7_store_feedback.py        # Store user feedback
â”‚   â”‚       â””â”€> Appends to data/feedback.jsonl
â”‚   â”‚
â”‚   â””â”€â”€ offline/                        # OFFLINE PHASE (training)
â”‚       â”‚
â”‚       â”œâ”€â”€ 1_prepare_samples.py       # Prepare training data
â”‚       â”‚   â””â”€> Load training_samples.json
â”‚       â”‚   â””â”€> Create sample data if needed
â”‚       â”‚
â”‚       â”œâ”€â”€ 2_compile_dspy.py          # Compile DSPy model
â”‚       â”‚   â””â”€> Load training samples
â”‚       â”‚   â””â”€> Use optimizer (MIPROv2/MIPRO/BootstrapFewShot)
â”‚       â”‚   â””â”€> Save compiled model to artifacts/
â”‚       â”‚
â”‚       â””â”€â”€ 3_recompile.py             # Recompile with feedback
â”‚           â””â”€> Merge feedback.jsonl into training_samples.json
â”‚           â””â”€> Run compile_dspy_model()
â”‚
â”œâ”€â”€ data/                               # Data storage
â”‚   â”œâ”€â”€ training_samples.json          # Training examples (100+)
â”‚   â””â”€â”€ feedback.jsonl                 # User feedback (append-only)
â”‚
â”œâ”€â”€ artifacts/                          # Model artifacts
â”‚   â””â”€â”€ compiled_rag_v1.dspy           # Compiled DSPy model
â”‚
â””â”€â”€ logs/                               # Application logs
```

---

## Request Flow Diagram

### Complete Flow: User Question â†’ Answer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. User sends question                                       â”‚
â”‚    POST /api/v1/ask                                          â”‚
â”‚    {"question": "What did WESTDALE do in June 2022?"}        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. FastAPI Route Handler (api/routes.py:ask_question)       â”‚
â”‚    â€¢ Validates request with QuestionRequest schema          â”‚
â”‚    â€¢ Extracts question string                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Get Context (handlers/runtime/2_get_context.py)          â”‚
â”‚    â€¢ POST to http://localhost:8000/retrieve                 â”‚
â”‚    â€¢ Payload: {"query": "...", "min_chunks": 3, "max": 6}   â”‚
â”‚    â€¢ Response: {"chunks": [{"text": "..."}, ...]}            â”‚
â”‚    â€¢ Extracts text from each chunk                          â”‚
â”‚    â€¢ Returns: List[str] with 3-6 context strings            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Get RAG Model (api/routes.py:get_rag_model)              â”‚
â”‚    â€¢ Checks if model is cached in _rag_model global var     â”‚
â”‚    â€¢ If not cached:                                          â”‚
â”‚      a. Configure OpenAI (handlers/runtime/4_configure...)   â”‚
â”‚         â””â”€> dspy.settings.configure(lm=dspy.OpenAI(...))    â”‚
â”‚      b. Load DSPy model (handlers/runtime/3_load_dspy.py)   â”‚
â”‚         â””â”€> Load compiled model OR create ReasonVerifyRAG() â”‚
â”‚    â€¢ Cache model for future requests                        â”‚
â”‚    â€¢ Returns: ReasonVerifyRAG instance                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Execute Reason + Verify (handlers/runtime/5_reason_...)  â”‚
â”‚                                                              â”‚
â”‚    Step 5a: REASON (LLM Call #1)                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚ â€¢ Signature: ReasonSignature                â”‚           â”‚
â”‚    â”‚ â€¢ Input: context + question                 â”‚           â”‚
â”‚    â”‚ â€¢ LLM generates DRAFT answer                â”‚           â”‚
â”‚    â”‚ â€¢ Uses ChainOfThought reasoning             â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚    Step 5b: VERIFY (LLM Call #2)                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚    â”‚ â€¢ Signature: VerifySignature                â”‚           â”‚
â”‚    â”‚ â€¢ Input: context + question + draft_answer  â”‚           â”‚
â”‚    â”‚ â€¢ LLM verifies and corrects draft           â”‚           â”‚
â”‚    â”‚ â€¢ Returns VERIFIED answer                   â”‚           â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                 â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Format Response (api/routes.py:ask_question)             â”‚
â”‚    â€¢ Extract verified_answer from result                    â”‚
â”‚    â€¢ Create AnswerResponse object                           â”‚
â”‚    â€¢ Include: question, answer, context_used                â”‚
â”‚    â€¢ Return JSON response                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Response to User                                          â”‚
â”‚    {                                                         â”‚
â”‚      "question": "What did WESTDALE do in June 2022?",      â”‚
â”‚      "answer": "In June 2022, WESTDALE entered into...",    â”‚
â”‚      "context_used": ["chunk1", "chunk2", ...],             â”‚
â”‚      "reasoning": null                                      â”‚
â”‚    }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## LLM Call Sequence

### Two LLM Calls Per Query

```
Question: "What did WESTDALE do in June 2022?"
Context: [3-6 chunks of retrieved text]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM CALL #1: REASON                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Signature: ReasonSignature                                  â”‚
â”‚ Method: dspy.ChainOfThought                                 â”‚
â”‚                                                             â”‚
â”‚ Input:                                                      â”‚
â”‚   â€¢ context: "TO: Bethany Shay, Westdale...\n\n..."        â”‚
â”‚   â€¢ question: "What did WESTDALE do in June 2022?"         â”‚
â”‚                                                             â”‚
â”‚ LLM Process:                                                â”‚
â”‚   1. Read and understand context                           â”‚
â”‚   2. Identify relevant information                         â”‚
â”‚   3. Generate draft answer                                 â”‚
â”‚                                                             â”‚
â”‚ Output:                                                     â”‚
â”‚   â€¢ answer: "In June 2022, Westdale entered into..."       â”‚
â”‚                                                             â”‚
â”‚ Cost: ~0.5Â¢ per 1000 tokens (gpt-4o-mini)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ draft_answer
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM CALL #2: VERIFY                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Signature: VerifySignature                                  â”‚
â”‚ Method: dspy.ChainOfThought                                 â”‚
â”‚                                                             â”‚
â”‚ Input:                                                      â”‚
â”‚   â€¢ context: "TO: Bethany Shay, Westdale...\n\n..."        â”‚
â”‚   â€¢ question: "What did WESTDALE do in June 2022?"         â”‚
â”‚   â€¢ draft_answer: "In June 2022, Westdale entered..."      â”‚
â”‚                                                             â”‚
â”‚ LLM Process:                                                â”‚
â”‚   1. Re-read context                                       â”‚
â”‚   2. Compare draft_answer with context                     â”‚
â”‚   3. Check for accuracy, completeness                      â”‚
â”‚   4. Correct any errors or omissions                       â”‚
â”‚   5. Generate verified answer                              â”‚
â”‚                                                             â”‚
â”‚ Output:                                                     â”‚
â”‚   â€¢ verified_answer: "In June 2022, WESTDALE entered..."   â”‚
â”‚                                                             â”‚
â”‚ Cost: ~0.5Â¢ per 1000 tokens (gpt-4o-mini)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ verified_answer
                         â–¼
                   Final Answer âœ“
```

**Total Cost Per Query**: ~$0.001 (less than 1 cent)

---

## Data Flow

### Runtime Data Flow (Per Query)

```
User Question
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "What did WESTDALE  â”‚
â”‚ do in June 2022?"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Context Retrieval          â”‚
â”‚ POST localhost:8000/retrieve       â”‚
â”‚                                    â”‚
â”‚ Request:                           â”‚
â”‚ {                                  â”‚
â”‚   "query": "What did WESTDALE...", â”‚
â”‚   "min_chunks": 3,                 â”‚
â”‚   "max_chunks": 6                  â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Response from Retrieval API:       â”‚
â”‚ {                                  â”‚
â”‚   "query": "...",                  â”‚
â”‚   "chunks": [                      â”‚
â”‚     {"text": "TO: Bethany...",     â”‚
â”‚      "document_id": "...", ...},   â”‚
â”‚     {"text": "POWELL COLEMAN...",  â”‚
â”‚      "document_id": "...", ...},   â”‚
â”‚     ...                            â”‚
â”‚   ]                                â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: Extract Text               â”‚
â”‚ context = [                        â”‚
â”‚   "TO: Bethany Shay...",           â”‚
â”‚   "POWELL COLEMAN...",             â”‚
â”‚   "â€¢Fidelity...",                  â”‚
â”‚   "ASSIGNMENT AND..."              â”‚
â”‚ ]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: Reason (LLM Call #1)       â”‚
â”‚                                    â”‚
â”‚ Input to OpenAI:                   â”‚
â”‚ {                                  â”‚
â”‚   "context": "TO: Bethany...\n\n"  â”‚
â”‚   "question": "What did..."        â”‚
â”‚ }                                  â”‚
â”‚                                    â”‚
â”‚ LLM Response:                      â”‚
â”‚ {                                  â”‚
â”‚   "answer": "In June 2022,         â”‚
â”‚              Westdale entered..."  â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Verify (LLM Call #2)       â”‚
â”‚                                    â”‚
â”‚ Input to OpenAI:                   â”‚
â”‚ {                                  â”‚
â”‚   "context": "TO: Bethany...\n\n"  â”‚
â”‚   "question": "What did..."        â”‚
â”‚   "draft_answer": "In June 2022..." â”‚
â”‚ }                                  â”‚
â”‚                                    â”‚
â”‚ LLM Response:                      â”‚
â”‚ {                                  â”‚
â”‚   "verified_answer": "In June..."  â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Final Response to User:            â”‚
â”‚ {                                  â”‚
â”‚   "question": "What did...",       â”‚
â”‚   "answer": "In June 2022...",     â”‚
â”‚   "context_used": [...],           â”‚
â”‚   "reasoning": null                â”‚
â”‚ }                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### 1. FastAPI Application (app.py)

**Purpose**: Main entry point for the application

**Key Features**:
- Creates FastAPI instance with title, description, version
- Configures CORS middleware (allows all origins)
- Includes routes from `api/routes.py`
- Exposes root endpoint with API information
- Runs on port 8001

**Code Location**: `/app.py`

---

### 2. API Routes (api/routes.py)

**Purpose**: Define all API endpoints

**Endpoints**:

| Endpoint | Method | Description | Handler |
|----------|--------|-------------|---------|
| `/api/v1/ask` | POST | Answer questions | `ask_question()` |
| `/api/v1/feedback` | POST | Store feedback | `submit_feedback()` |
| `/api/v1/compile` | POST | Compile DSPy | `compile_model()` |
| `/api/v1/recompile` | POST | Recompile with feedback | `recompile_model()` |
| `/api/v1/health` | GET | Health check | `health_check()` |

**Code Location**: `/api/routes.py`

---

### 3. Context Retrieval (handlers/runtime/2_get_context.py)

**Purpose**: Retrieve relevant context chunks from external API

**Process**:
1. Receives question from user
2. Calls external retrieval API at `localhost:8000/retrieve`
3. Sends: `{"query": question, "min_chunks": 3, "max_chunks": 6}`
4. Receives: `{"chunks": [{"text": "...", ...}, ...]}`
5. Extracts "text" field from each chunk
6. Returns: `List[str]` with 3-6 context strings

**Configuration**:
- Uses `RETRIEVAL_API_URL` env var (defaults to `localhost:8000`)
- Timeout: 30 seconds
- Returns empty list on error

**Code Location**: `/handlers/runtime/2_get_context.py`

---

### 4. DSPy Model Loading (handlers/runtime/3_load_dspy.py)

**Purpose**: Load compiled DSPy model or create new instance

**Process**:
1. Check if compiled model exists at `artifacts/compiled_rag_v1.dspy`
2. If exists:
   - Load using `dspy.load(model_path)`
   - Return compiled model
3. If not exists or load fails:
   - Import `ReasonVerifyRAG` class
   - Create new instance
   - Return uncompiled model

**Code Location**: `/handlers/runtime/3_load_dspy.py`

---

### 5. OpenAI Configuration (handlers/runtime/4_configure_model.py)

**Purpose**: Configure DSPy to use OpenAI

**Process**:
1. Read settings from `config/settings.py`
2. Create `dspy.OpenAI` instance with:
   - Model: `gpt-4o-mini`
   - API Key: from `.env`
   - Temperature: `0.0`
3. Configure DSPy globally: `dspy.settings.configure(lm=lm)`

**Code Location**: `/handlers/runtime/4_configure_model.py`

---

### 6. Reason + Verify Pipeline (handlers/runtime/5_reason_verify.py)

**Purpose**: Two-stage RAG pipeline with reasoning and verification

**Components**:

**ReasonSignature**:
- Input: context (str), question (str)
- Output: answer (str)
- Purpose: Generate draft answer

**VerifySignature**:
- Input: context (str), question (str), draft_answer (str)
- Output: verified_answer (str)
- Purpose: Verify and correct draft

**ReasonVerifyRAG Class**:
- Inherits from `dspy.Module`
- Method: `forward(context, question)`
- Process:
  1. Join context list into single string
  2. Call `self.reason()` â†’ LLM call #1
  3. Call `self.verify()` â†’ LLM call #2
  4. Return verified result

**Code Location**: `/handlers/runtime/5_reason_verify.py`

---

### 7. Feedback Storage (handlers/runtime/7_store_feedback.py)

**Purpose**: Store user feedback for future training

**Process**:
1. Receive: question, context, answer, liked (bool)
2. If `liked=True`:
   - Create JSON entry
   - Append to `data/feedback.jsonl`
3. If `liked=False`:
   - Ignore (don't store negative feedback)

**Code Location**: `/handlers/runtime/7_store_feedback.py`

---

## Training Phase (Optional)

### Offline Compilation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Prepare Training Data                                    â”‚
â”‚    â€¢ Create data/training_samples.json                      â”‚
â”‚    â€¢ Format: [{"question": "...", "context": [...],         â”‚
â”‚                "answer": "..."}, ...]                       â”‚
â”‚    â€¢ Need: 100+ examples                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Run Compilation                                          â”‚
â”‚    $ python handlers/offline/2_compile_dspy.py              â”‚
â”‚                                                             â”‚
â”‚    Process:                                                 â”‚
â”‚    â€¢ Load training_samples.json                             â”‚
â”‚    â€¢ Configure OpenAI                                       â”‚
â”‚    â€¢ Create ReasonVerifyRAG instance                        â”‚
â”‚    â€¢ Initialize optimizer (MIPROv2/MIPRO/BootstrapFewShot)  â”‚
â”‚    â€¢ Run optimizer.compile(rag, trainset)                   â”‚
â”‚    â€¢ Makes 300-600 LLM calls to optimize prompts            â”‚
â”‚    â€¢ Save to artifacts/compiled_rag_v1.dspy                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Compiled Model Created                                   â”‚
â”‚    â€¢ File: artifacts/compiled_rag_v1.dspy                   â”‚
â”‚    â€¢ Optimized prompts for Reason + Verify                  â”‚
â”‚    â€¢ Improved accuracy and consistency                      â”‚
â”‚    â€¢ API will auto-use this on next restart                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | Required | OpenAI API key |
| `OPENAI_MODEL` | `gpt-4o-mini` | OpenAI model name |
| `OPENAI_TEMPERATURE` | `0.0` | Temperature for LLM |
| `RETRIEVAL_API_URL` | `http://localhost:8000` | Retrieval API base URL |

---

## Performance Metrics

| Metric | Value |
|--------|-------|
| LLM Calls per Query | 2 |
| Average Latency | 2-4 seconds |
| Cost per Query | < $0.001 |
| Context Chunks | 3-6 |
| Max Context Length | ~6000 tokens |
| Compilation Cost | $0.50-1.00 (one-time) |

---

## Next Steps

1. **Test the API**: `curl http://localhost:8001/api/v1/health`
2. **Ask Questions**: See QUICKSTART.md
3. **Deploy**: See DEPLOYMENT.md
4. **Train Model**: When you have 100+ examples, compile DSPy

---

## Support

- **API Docs**: http://localhost:8001/docs
- **Logs**: `docker logs -f dspy-rag-api`
- **Issues**: Check error messages in logs
